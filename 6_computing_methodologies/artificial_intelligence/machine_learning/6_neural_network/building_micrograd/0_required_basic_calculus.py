# %% [markdown]
# """
# references:
#     Karpathy, A. 2022. The spelled-out intro to neural networks and backpropagation: building micrograd. https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ
# """

# %%
import math
import numpy as np
import matplotlib.pyplot as plt

# %%
'''
scalar valued function
input = x
ouput = y
where x AND y are scalar values
'''
def f(x):
    return 3*x**2 - 4*x + 5 # 3xÂ² - 4x + 5

# %%
f(3.0)

# %%
x_scalar_values = np.arange(-5, 5, 0.25)
y_scalar_values = f(x_scalar_values)
plt.plot(x_scalar_values, y_scalar_values)

# %%
# what is the derivative of this function(f(x)) at any input value x

# slope at 3
h = 0.00000001
x = 3.0
how_much_the_function_responded = f(x + h) - f(x)
'''
    normalize by run
        = rise/run
'''
slope = how_much_the_function_responded/h
slope # numerial approximation. requires h to be very small to get the actual scope

# %%
# slope at -3
h = 0.00000001
x = -3.0
how_much_the_function_responded = f(x + h) - f(x)
slope = how_much_the_function_responded / h
slope

# %%
# slope at 2/3
# i.e. the function does not respond
h = 0.00000001
x = 2/3
how_much_the_function_responded = f(x + h) - f(x)
slope = how_much_the_function_responded / h
slope 
'''
    slope = 0 i.e the function does not respond
    stay almost the same?
'''

# %%
# another function scalar function
# from 3 scalars to 1 scalar
a = 2.0
b = -3.0
c = 10.0
def f(a, b, c):
    return a * b + c

d = f(a, b, c)
d

# %%
a_values = np.arange(-5, 5, 0.25)
b_values = np.arange(-5, 5, 0.25)
c_values = np.arange(-5, 5, 0.25)
d_values = f(a_values, b_values, c_values)
plt.plot(d_values)

# %%
h = 0.00000001
a = 2.0
b = -3.0
c = 10.0

d1 = f(a, b, c)
d2 = f(a + h, b, c)
print('d1 = ', d1)
print('d2 = ', d2)
print('slope = ', (d2 - d1)/h)


# %% [markdown]
# 


